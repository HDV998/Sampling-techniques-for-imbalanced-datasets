# Sampling Techniques for Imbalanced Datasets

## Project Overview
This project focuses on analyzing the impact of different sampling techniques on the performance of various machine learning models when dealing with highly imbalanced datasets. The primary objective is to balance a credit card fraud detection dataset, apply five distinct sampling strategies, and evaluate five different machine learning models to determine the optimal combination for accuracy.

## Objective
*   To understand the importance of sampling techniques in handling imbalanced datasets.
*   To compare how different sampling strategies (e.g., Simple Random, Stratified, Systematic, etc.) affect model accuracy.
*   To determine which sampling technique yields the highest accuracy for specific ML models.

## Dataset
The project uses a highly imbalanced **Credit Card Fraud Detection** dataset.
*   **Source:** [GitHub Repository - Creditcard_data.csv](https://github.com/AnjulaMehto/Sampling_Assignment/blob/main/Creditcard_data.csv)
*   **Class Distribution:** The original dataset contains a majority class (legitimate transactions) and a minority class (fraudulent transactions).

## Methodology

### 1. Data Preprocessing
*   **Balancing:** The initial imbalanced dataset was converted into a balanced dataset to ensure fair training.
*   **Sample Generation:** From the balanced dataset, five different samples were created using distinct sampling techniques (referred to as Sampling1 through Sampling5).

### 2. Sampling Techniques Used
Based on the implementation, the following sampling strategies were applied:
*   **Sampling1:** **Simple Random Sampling**
*   **Sampling2:** **Systematic Sampling** 
*   **Sampling3:** **Stratified Sampling** 
*   **Sampling4:** **Cluster Sampling** 
*   **Sampling5:** **Bootstrap Sampling** 

### 3. Machine Learning Models
Five different classifiers were trained on each of the five samples:
*   **M1:** **Logistic Regression** 
*   **M2:** **Decision Tree Classifier** 
*   **M3:** **Random Forest Classifier** 
*   **M4:** **K-Nearest Neighbors (KNN)** 
*   **M5:** **Na√Øve Bayes** 

## Implementation Details
 The project involves an automated training loop that:
1.  Iterates through every Model and Sampling combination.
2.  Splits the data into training and testing sets (75/25 split).
3.  **Error Handling:** Checks if the target class in the sample contains only one unique value (Pure Sample). If so, training is skipped for that specific combination to avoid model errors.

## Results
The following table shows the accuracy scores obtained for each model across different sampling techniques.

| Model | Sampling1 | Sampling2 | Sampling3 | Sampling4 | Sampling5 |
|-------|-----------|-----------|-----------|-----------|-----------|
| **M1**| 0.7835    | 0.9381    | 0.8750    | NaN       | 0.9278    |
| **M2**| 0.9588    | 0.9588    | 0.9792    | NaN       | 0.9897    |
| **M3**| 1.0000    | 1.0000    | 1.0000    | NaN       | 1.0000    |
| **M4**| 0.9691    | 0.9072    | 0.9583    | NaN       | 0.9278    |
| **M5**| 0.8247    | 0.6495    | 0.7292    | NaN       | 0.7423    |

### Key Observations
*   **Sampling4 Issue:** All models returned `NaN` for **Sampling4**. As seen in the execution logs, training was skipped because the sample generated by this specific technique resulted in a target class with only one unique value (no variance in the target variable).
*   **Best Model:** **M3** achieved a perfect accuracy score of **1.00** across all successful sampling techniques, suggesting it is the most robust model for this specific balanced dataset.
*   **Best Sampling Technique:** 
    *   For **M1**, Sampling2 provided the highest accuracy (0.9381).
    *   For **M2**, Sampling5 provided the highest accuracy (0.9897).

## Discussion
Based on the analysis:
1.  **M3** is the superior model for this dataset, showing no sensitivity to the variations between Sampling 1, 2, 3, and 5.
2.  **Sampling4** proved ineffective in this specific run as it failed to capture representative data from both classes, leading to a single-class sample.
3.  **M5** showed the highest volatility, with accuracy ranging from ~65% (Sampling2) to ~82% (Sampling1), indicating it is highly sensitive to the sampling method used.

## How to Run
1.  Install dependencies:
    ```bash
    pip install pandas numpy scikit-learn
    ```
2.  Download the dataset from the link provided above.
3.  Run the Python script/Notebook to generate the balanced samples and train the models.
4.  The output will generate a CSV file `sampling_results.csv` containing the accuracy matrix.
